\documentclass{amsart}
\usepackage{mathtools}
\usepackage[dutch]{babel}
\usepackage[T1]{fontenc}

%%% ENV

\theoremstyle{definition}
\newtheorem{axm}{Axioma}[section]
\newtheorem{thm}{Stelling}[section]
\newtheorem{lmm}{Lemma}[section]
\newtheorem{dfn}{Definitie}[section]
\newtheorem{csq}{Gevolg}[section]

\newenvironment{bewijs}{\begin{proof}[Bewijs]}{\end{proof}}



%%% CMD

%% sets

\newcommand{\setsm}[1]{\{{#1}\}}
\newcommand{\without}[1]{\setminus\setsm{#1}}
\newcommand{\with}[1]{\cup\setsm{#1}}

\newcommand{\realnums}{\mathbb{R}}
\newcommand{\realn}[1][n]{\realnums^{#1}}
\newcommand{\realmx}[2][n]{\realn[#2 \times #1]}
\newcommand{\realnxn}{\realmx{n}}
\newcommand{\realmxn}{\realmx{m}}

\newcommand{\vecspace}[1][v]{\mathrm{\mathbf{\MakeUppercase#1}}}
\newcommand{\vecspacen}[1][n]{\vecspace^#1}


%% numbers

\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\abs}[1]{\lvert{#1}\rvert}


%% vectors

% bold vectors
\newcommand{\vvec}[1][v]{\mathbf{#1}}
\newcommand{\uvec}[1][v]{\vvec[#1]_\mathbf{e}}

\newcommand{\vnorm}[1]{\norm{\vvec[#1]}}
\newcommand{\zerovec}{\vvec[0]}

% vector rows and their vectors
\newcommand{\vecrow}[1][a]{\mathfrak{\MakeUppercase{#1}}}

\newcommand{\rvec}[2][i]{{#2}_{#1}}
\newcommand{\rvecr}[2][i]{\rvec[#1]{\vecrow[#2]}}
\newcommand{\rveci}[1][i]{\rvecr[#1]{a}}

% coordinate vectors
\newcommand{\cvec}[2]{{#1}^{#2}}

\newcommand{\cveca}[2][a]{\cvec{#2}{\vecrow[#1]}}

\newcommand{\cvecv}[2][v]{\cvec{\vvec[#1]}{#2}}
\newcommand{\cvecva}[1][a]{\cvecv{\vecrow[#1]}}
\newcommand{\cvecc}[2][a]{\cvecv[#2]{\vecrow[#1]}}
\newcommand{\cvecvv}[1][v]{\cvecc{#1}}

% coordinates
\newcommand{\vcord}[3]{{#1}_{#2}^{#3}}

\newcommand{\vcorda}[3][a]{\vcord{#2}{#3}{\vecrow[#1]}}
\newcommand{\vcordai}[2][i]{\vcorda{#2}{#1}}

\newcommand{\vcordv}[3][v]{\vcord{#1}{#2}{\vecrow[#3]}}
\newcommand{\vcordvi}[2][i]{\vcordv{#1}{#2}}

\newcommand{\vcordvia}[1][i]{\vcordvi[#1]{a}}


%%% DOC

\begin{document}
\title{Lineaire algebra}
\author{Olesj V. Bilous}
\dedicatory{toegewijd aan Ganna}
%\begin{abstract}
%\end{abstract}
\maketitle

\section{Vectorruimtes}


\begin{dfn}
	Een vectorruimte $\vecspace$ bevat van elke vector $\vvec \in \vecspace$ een unieke schaal $\lambda \vvec$ voor elke scalair $\lambda \in \realnums$. Het bevat ook de unieke som $\vvec + \vvec[w]$ van elk paar vectoren $\vvec, \vvec[w] \in \vecspace$.
\end{dfn}

\begin{axm}
	De vectorschaal is associatief.
	\begin{equation*}
		\lambda (\mu \vvec) = (\lambda\mu)\vvec
	\end{equation*}
\end{axm}

\begin{axm}
	$1\vvec$ is de identiteitsschaal.
	\begin{equation*}
		1\vvec=\vvec
	\end{equation*}
\end{axm}

\begin{axm}
	Er bestaat een unieke nulvector $\zerovec$ die de nulschaal is van elke vector $\vvec \in \vecspace$.
	\begin{equation*}
		0\vvec=\zerovec
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorsom is associatief.
	\begin{equation*}
		(\vvec[u] + \vvec) + \vvec[w] = \vvec[u] + (\vvec + \vvec[w])
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorsom is commutatief.
	\begin{equation*}
		\vvec + \vvec[w] = \vvec[w] + \vvec
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorschaal is distributief over de vectorsom.
	\begin{equation*}
		\lambda(\vvec + \vvec[w]) = \lambda \vvec + \lambda \vvec[w]
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorschaal is distributief over de scalaire som.
	\begin{equation*}
		(\lambda + \mu)\vvec = \lambda \vvec + \mu \vvec
	\end{equation*}
\end{axm}

\begin{thm}
	Een vectorruimte is een commutatieve groep over de vectorsom.
	\begin{bewijs}
		\begin{align*}
			\vvec + 0\vvec    & = (1+0)\vvec \\
			\vvec + (-1)\vvec & = (1-1)\vvec
		\end{align*}
	\end{bewijs}
\end{thm}

\begin{dfn}
	De nulruimte van een vectorruimte $\vecspace_0 = \setsm\zerovec \subseteq \vecspace$ bevat enkel de nulvector.
\end{dfn}

\begin{csq}
	De nulruimte is een triviale vectorruimte.
\end{csq}

\begin{csq}
	Een eindige deelverzameling van een vectorruimte is enkel een vectorruimte als het de triviale vectorruimte is.
\end{csq}

\begin{dfn}
	De lineaire combinatie van een rij vectoren $\rveci \in \vecrow \in \vecspacen$ volgens een kolom coëfficiënten $\vcordvia \in \cvecva \in \mathbb{R}^n$
	schaalt elke vector in de rij met de respectieve coëfficiënt in de kolom en somt over deze schalen.
	\begin{align*}
		\vecrow \cvecva & = \sum_i^n \vcordvia \rveci
		\\ & = \vcordvia[1] \rveci[1] + \ldots + \vcordvia[n] \rveci[n]
		\\ & =
		\begin{pmatrix}
			\rveci[1], \ldots, \rveci[n]
		\end{pmatrix}
		\begin{pmatrix}
			\vcordvia[1] \\
			\vdots       \\
			\vcordvia[n]
		\end{pmatrix}
	\end{align*}
\end{dfn}

\begin{csq}
	Zo is elke lineaire combinatie van vectoren $\rveci \in \vecspace$ ook een vector $\vecrow \cvecva = \vvec \in \vecspace$.
\end{csq}

\begin{dfn}
	De schaal van een kolom coëfficiënten $v_i \in \vvec \in \realn$ is de kolom $\vvec[w] \in \realn$ met de schalen van de respectieve coëfficiënten ${w}_i = \lambda {v}_i$.
	\begin{align*}
		\lambda \vvec =
		\lambda
		\begin{pmatrix}
			v_1    \\
			\vdots \\
			v_n
		\end{pmatrix}
		=
		\begin{pmatrix}
			\lambda v_1 \\
			\vdots      \\
			\lambda v_n
		\end{pmatrix}
	\end{align*}
\end{dfn}

\begin{dfn}
	De som van coëfficiëntkolommen $\vvec[u], \vvec \in \realn$ is de kolom $\vvec[w] \in \realn$ met de sommen van de respectieve coëfficiënten ${w}_i = {u}_i + {v}_i$.
	\begin{align*}
		\vvec[u] + \vvec =
		\begin{pmatrix}
			u_1    \\
			\vdots \\
			u_n
		\end{pmatrix}
		+
		\begin{pmatrix}
			v_1    \\
			\vdots \\
			v_n
		\end{pmatrix}
		=
		\begin{pmatrix}
			u_1 + v_1 \\
			\vdots    \\
			u_1 + v_n
		\end{pmatrix}
	\end{align*}
\end{dfn}

\begin{thm}
	$\realn$ is een vectorruimte met nulvector $0_n$ waar elke coëfficiënt nul is.
\end{thm}

Vandaar dat men een kolom coëfficiënten doorgaans een kolomvector zal noemen. De coëfficiënten heten dan componenten.

\begin{dfn}
	De vectorrij $A \in \realn[m \times n]$ met $n$ kolomvectoren $A_{j} \in \realn[m]$ noemt men een matrix.

	Evenwel vormt de matrix $A$ ook een vectorkolom met $m$ rijvectoren $A^i \in \realn$.

	De gemene component tussen $A_j$ en $A^i$ noemt men een element van de matrix, geschreven $a_{ij}$.
\end{dfn}

\begin{dfn}
	Transpositie $A^T \in \realmx[m]{n}$ spiegelt een matrix $A \in \realmxn$ om de diagonaal $a^T_{ij} = a_{ji}$ zodat rijen kolommen worden $(A^T)_j = (A^j)^T$ en vice versa.
\end{dfn}

\begin{csq}
	Herhaalde lineaire combinaties van een vectorrij $\vecrow \in \vecspacen[m]$ volgens de kolomvectoren $B_i$ van een matrix $B \in \realmxn$ vormen een vectorrij $\vecrow[b] \in \vecspacen$.
\end{csq}

\begin{dfn}
	Bovenstaande operatie noemen we de lineaire transformatie van $\vecrow$ volgens $B$.
	\begin{equation*}
		\vecrow B = \vecrow[b]
	\end{equation*}
\end{dfn}

\begin{dfn}
	De matrix die een vectorrij op zichzelf afbeeldt noemen we de identiteitsmatrix $I_n \in \realnxn$ zodat $\vecrow I_n = \vecrow$.
\end{dfn}

\begin{csq}
	De lineaire transformatie van de $m$ kolomvectoren $A_i \in \realn[k]$ van de matrix $A \in \realmx[m]{k}$ volgens de matrix $B \in \realmxn$ bestaat uit de $n$ kolomvectoren $C_j \in \realn[k]$ van de matrix $C \in \realmx[n]{k}$.
\end{csq}

\begin{dfn}
	Dit staat ook simpelweg als het matrixproduct bekend.
	\begin{equation*}
		AB = C
	\end{equation*}
\end{dfn}

\begin{csq}
	Elk van de $k$ rijvectoren $C^h \in \realn$ bestaat anderzijds uit een lineaire combinatie van $m$ rijvectoren $B^i \in \realn$ volgens de rijvector $A^h \in \realn[m]$, oftewel $(AB)^T = B^TA^T$.
\end{csq}

\begin{csq}
	Het matrixproduct is associatief $(AB)C = A(BC)$.
	\begin{bewijs}
		De vectorschaal is distributief over de vectorsom en bovendien associatief.
		Vandaar dat het in $ABC$ niet uitmaakt of een $C_i$ zijn coëfficiënten verdeelt over $AB$ of een $(BC)_j$ de zijne over $A$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Inverteerbare vierkante matrices $A \in \realnxn$ zijn commutatief inverteerbaar.
	\begin{bewijs}
		Weze $AA^{-1}_r = I_n$ in $(AA^{-1}_r)A = A(A^{-1}_rA)$. Hierbij moet $A^{-1}_rA$ echter ook dezelfde identiteitsmatrix $I_n$ zijn.
	\end{bewijs}
\end{csq}

\begin{dfn}
	De spanruimte $span(\vecrow)$ van een vectorrij $\vecrow$ bevat alle linaire combinaties van $\vecrow$.
	Men zegt dat $\vecrow$ haar spanruimte voortbrengt, of een voortbrengend deel is.
\end{dfn}

\begin{dfn}
	De spanruimte van de kolomvectoren $A_j$ in een matrix $A$ noemt men de kolomruimte van de matrix.
	De spanruimte van de rijvectoren $A^i$ heet de rijruimte.
\end{dfn}

\begin{csq}
	Een matrix $A \in \realmxn$ met kolomruimte $\realn[m]$ is rechts inverteerbaar.
	Met rijruimte $\realn$ is ze links inverteerbaar.
	\begin{bewijs}
		De identiteitsmatrices bestaan uit vectoren in de respectieve ruimtes.
	\end{bewijs}
\end{csq}

\begin{thm}
	De spanruimte van een vectorrij is een vectorruimte.
	\begin{bewijs}
		Uit $\vecrow B = \vecrow[b]$ en $\vecrow[b] v^{\vecrow[b]}= \vvec$ volgt $\vecrow B v^{\vecrow[b]}= \vvec$.
		Hieruit mag blijken dat herhaalde lineaire combinaties de spanruimte niet verlaten.
	\end{bewijs}
\end{thm}

\section{Vrije delen}

Evenwel bestaan er vectorruimtes $\vecspace$ waar niet elke vector $\vvec \in \vecspace$ een lineaire combinatie van elke vectorrij $\vecrow \in \vecspacen$ is.

\begin{dfn}
	Een vectorrij $\vecrow$ is lineair onafhankelijk of vrij als geen enkele van de vectoren $\vecrow_i \in \vecrow$ een lineaire combinatie is van de overige $\vecrow_{j\neq i} \in \vecrow$.
	Een dergelijke rij staat ook gekend als een vrij deel.
\end{dfn}

\begin{lmm}
	De lineaire combinatie van een vrij deel volgens een kolomvector is enkel de nulvector $\vecrow \cvecvv = \zerovec$ als de kolomvector dat ook is in zijn respectieve ruimte $\cvecvv = 0_n$.
	\begin{bewijs}
		$-\vcordvia \vecrow_i = \left(\vecrow \without{\rveci}\right)\left(\cvecvv \without{\vcordvia}\right)$.
	\end{bewijs}
\end{lmm}

\begin{csq}
	De transformatie van een vrije vectorrij volgens een matrix is vrij als de kolommen van de matrix dat ook zijn. Als de transformatie van een willekeurige vectorrij volgens een matrix vrij is, zijn de kolommen van de matrix dat ook.
	\begin{bewijs}
		Stel dat $\vecrow[b] = \vecrow B$ lineair afhankelijk is waar $B \in \realmxn$ en $\vecrow \in \vecspacen[m]$ vrij.
		Dan is er een $\cvecc[b]{0} \neq 0_n$ waaruit $B \cvecc[b]{0} = 0_n$ gezien de vrijheid van $\vecrow$, zodat ook $B$ lineair afhankelijk blijkt.
		Anderzijds is voor $D\vvec = 0_k$ elke $\vecrow[c] D\vvec = \zerovec$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Het product van matrices met vrije kolommen heeft vrije kolommen. Als het product van willekeurige matrices vrije kolommen heeft, heeft de rechtermatrix dat ook.
\end{csq}

\begin{csq}
	Een vrije vectorrij heeft voor elke vector in zijn spanruimte slechts één kolom coëfficiënten.
	\begin{bewijs}
		\begin{align*}
			\vecrow \vvec              & = \vecrow \vvec[w] \\
			\vecrow (\vvec - \vvec[w]) & = \zerovec
		\end{align*}
	\end{bewijs}
\end{csq}

\begin{dfn}
	Vanwege het unieke karakter van deze coëfficiënten noemen we ze de coördinaten van een vector in de spanruimte met betrekking tot de vrije vectorrij.
\end{dfn}

\begin{dfn}
	Een vrije vectorrij staat gekend als een basis van haar spanruimte.
\end{dfn}

\begin{dfn}
	De coördinaten van een alternatieve basis ten aanzien van een oorspronkelijke vormen een matrix.
	Deze heet een basistransformatie.
\end{dfn}

\begin{thm}
	Als $\vecrow[b]$ vrij wordt voortgebracht $span(\vecrow[b]) \subseteq span(\vecrow)$ uit $\vecrow$ dan bevat $\vecrow[b]$ ten hoogste een gelijk aantal vectoren $\abs{\vecrow[b]} \leq \abs\vecrow$.
	Als $\abs{\vecrow[b]} = \abs\vecrow$ dan is $\vecrow[b]$ voortbrengend $span(\vecrow[b]) = span(\vecrow)$ en $\vecrow$ vrij.
	\begin{bewijs}
		Elke $\vecrow[b]_j$ heeft minstens één definiete coëfficiënt $b_{ij} \in B$ voor een zekere $\vecrow_i$.
		\begin{align*}
			\vecrow[b]_j = b_{ij}\vecrow_i + (\vecrow \without{\vecrow_i})(B_j  \without{b_{ij}})
		\end{align*}
		Hieruit blijkt echter dat ook deze $\vecrow_i$ afhangt van  $\vecrow^{(1)} = \left(\vecrow \without{\vecrow_i}\right) \cup \rvecr[j]{b}$.
		Vandaar dat $\vecrow^{(1)}$ voortbrengend blijft gezien $span(\vecrow) \subseteq span(\vecrow^{(1)})$.

		Daar $\vecrow[b]$ vrij is en voortgebracht door $\vecrow^{(m)}$ heeft elke $\rvecr[k]{b} \notin \vecrow^{(m)}$ telkens een definiete coëfficiënt naar een vector in $\vecrow^{(m)}\without{\vecrow[b]}$
		en is deze procedure voor herhaling vatbaar tot we $\vecrow$ uitputten.

		Blijft er nog enige $\vecrow[b]_l \notin \vecrow^{(\abs{\vecrow})}$ over,
		dan is deze lineair afhankelijk van $\vecrow^{(\abs\vecrow)} \subset \vecrow[b]$ en ware $\vecrow[b]$ nimmer vrij.

		Geldt evenwel $\vecrow^{(\abs{\vecrow})} = \vecrow[b]$ dan is $\vecrow[b]$ voortbrengend.
		Ware $\vecrow$ nu niet vrij, dan kon ze ingeperkt worden zonder aan de voortbrengendheid te schaden.
		We toonden evenwel juist aan dat een voortbrengend deel niet kleiner kan zijn dan het vrij voortgebrachte.
	\end{bewijs}
\end{thm}

\begin{csq}
	Een vectorrij $\vecrow$ van lagere orde $\abs\vecrow < \abs{\vecrow[b]}$ dan een basis $\vecrow[b]$ is niet voortbrengend voor de spanruimte van de basis $span(\vecrow) \subset span(\vecrow[b])$.
\end{csq}

\begin{csq}
	Elk basispaar dat dezelfde vectorruimte voortbrengt $span(\vecrow) = span(\vecrow[b])$ heeft gelijke orde $\abs\vecrow = \abs{\vecrow[b]}$.
\end{csq}

\begin{dfn}
	We noemen deze orde de dimensie van de vectorruimte $dim(\vecspace)$.
\end{dfn}

\begin{csq}
	Enkel vierkante matrices kunnen commutatief inverteerbaar zijn.
	\begin{bewijs}
		Weze $A \in \realmxn$ met $m < n$ doch $\realn[m]$ voortbrengend.
		Dan kan $B \in \realmx[m]{n}$ weliswaar $I_m$ vormen uit $A$, maar $A$ kan $\realn$ niet voortbrengen uit $B$ om tot $I_n$ te komen.
	\end{bewijs}
\end{csq}

\begin{csq}
	De volgende zijn gelijkwaardig voor vierkante matrices $A \in \realnxn$:

	- vrijheid van de kolomvectoren

	- vrijheid van de rijvectoren

	- de kolomruimte is $\realn$

	- de rijruimte is $\realn$

	- inverteerbaarheid
\end{csq}

\begin{thm}
	Basistransformaties vormen een groep over de matrixvermenigvuldiging met de identiteitsmatrix als identiteit: $AA^{-1} = I_n = A^{-1}A$.
\end{thm}

\begin{dfn}
	Men noemt deze de algemene lineaire groep $GL_n(\realnums)$ met dimensie $n$ over het reële veld.
\end{dfn}

\begin{csq}
	Een basistransformatie is contravariant aan de ge\"{i}mpliceerde co\"{o}rdinatentransformatie.
	\begin{bewijs}
		Uit $\vecrow B = \vecrow[b]$ en $\vecrow \cvecva= \vvec$ volgt $\vecrow[b] B^{-1} \cvecva = \vvec$ zodat $B^{-1} \cvecva = \cvecva[b]$.
	\end{bewijs}
	Evenwel betekent dit slechts dat de overeenkomstige matrices elkaars inverse zijn.
	De wijze waarop een matrix inwerkt op een vectorrij verschilt immers van de werking op een kolom coördinaten, waarbij commutativiteit niet gegarandeerd is.
\end{csq}

\section{Genormeerde ruimtes}

Nu wensen we een begrip van afstand in te voeren, gekend als de norm $\vnorm v$ van een vector $\vvec \in \vecspace$.
Hiertoe nemen we een willekeurige basis $\vecrow[t]$ van $\vecspace$ en schrijven er enkele bijzondere eigenschappen aan toe.
\begin{axm}
	Vooreerst is telkens $\norm{\rvecr{t}} = 1$.
\end{axm}

\begin{axm}
	Vervolgens is de schaal associatief met de norm: $\norm{\lambda \vvec} = \lambda\vnorm v$.
\end{axm}

\begin{csq}
	Hieruit mag blijken dat er voor elke $\vvec \in \vecspace$ een vector $\uvec = {\vnorm v}^{-1}\vvec$ bestaat waarvan de norm $\vnorm \uvec = 1$.
\end{csq}

Beschouw een vrij paar vectoren $\vvec, \vvec[w]$ dat een derde vector $\vvec[c]$ voortbrengt zodat $\vvec[c] = \lambda \vvec + \mu \vvec[w]$.

\begin{dfn}
	We noemen $\lambda\vvec$ de parallele projectie van $\vvec[c]$ op de richting van $\vvec$ volgens de richting van $\vvec[w]$ geschreven $proj_{\vvec[w]}(\vvec[c], \vvec)$.
\end{dfn}

\begin{csq}
	De parallelle projectie van een vector is gelijk aan de som van de parallele projecties van de samenstellende componenten.
	\begin{bewijs}
		Weze $\vvec[c] = \sum_{i} {\vvec[u]}_i$ voor willekeurige vectoren $\vvec[u]_i$.
		Elke ${\vvec[u]}_i$ kan geschreven worden als een lineaire combinatie van een basis die $\vvec$, $\vvec[w]$ bevat, waarbij ze dus een coördinaat $\nu_i$ hebben naar $\vvec$.
		De coördinaat van $\vvec[c]$ naar $\vvec$ blijft $\lambda = \sum_i \nu_i$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Als $proj_{\vvec[w]}(\vvec[c], \vvec) = \zerovec$ dan ligt $\vvec[c]$ in de richting van projectie.
	\begin{bewijs}
		Daar $\lambda\vvec$ de nulvector blijkt is $\vvec[c] = \mu\vvec[w]$ een schaal van $\vvec[w]$.
	\end{bewijs}
\end{csq}

Stel voortaan $\vvec[a] = proj_{\vvec[w]}(\vvec[c], \vvec)$ en $\vvec[b] = proj_{\vvec}(\vvec[c], \vvec[w])$.

\begin{csq}
	Merk op dat $\vnorm{a}$ de coördinaat is van $\vvec[c]$ naar $\uvec[a]$ in het basispaar $\uvec[a], \vvec[w]$.
	Vandaar dat we $\vvec[a]$ ook de component van $\vvec[c]$ naar $\uvec[a]$ langs $\vvec[w]$ noemen.
\end{csq}

Evenwel kunnen we nu ook volgende vectoren beschouwen, $\vvec[c_a] = \vnorm a \uvec[c]$ en $\vvec[a_c] = \vnorm c \uvec[a]$.
Hierbij spiegelen we de driehoek gevormd door $\vvec[a], \vvec[c]$ om de bissectrice en voltooien die met $\vvec[b'] = \vvec[c_a] - \vvec[a_c]$.

Analoog vormen we $\vvec[a'] = \vnorm b\uvec[c] - \vnorm c\uvec[b]$.

\begin{dfn}
	Evenwel wensen wij nu $\vvec[a]$ volgens $\vvec[b']$ te projecteren op $\vvec[c]$, hetgeen we de terugkerende projectie van $\vvec[c]$ naar $\vvec$ langs $\vvec[w]$ noemen $reproj_{\vvec[w]}(\vvec[c], \vvec) = proj_{\vvec[b']}(\vvec[a], \vvec[c])$.
\end{dfn}

Blijkt evenwel uit de definitie van $\vvec[b']$ dat projecties langs $\vvec[b']$ op $\vvec[c]$ van vectoren langs $\uvec[a]$ schalen met een factor van $\frac{\vnorm a}{\vnorm c}$.

\begin{csq}
	\begin{equation*}
		\norm{reproj_{\vvec[w]}(\vvec[c], \vvec)} = \frac{\norm{proj_{\vvec[w]}(\vvec[c], \vvec)}^2}{\vnorm c}
	\end{equation*}
\end{csq}

\begin{dfn}
	Weze $\vnorm c$ = $\norm{reproj_{\vvec[w]}(\vvec[c], \vvec)} + \norm{reproj_{\vvec}(\vvec[c], \vvec[w])}$ dan noemt men $\vvec$ en $\vvec[w]$ onderling rechthoekig $\vvec \perp \vvec[w]$.
\end{dfn}

Merk op dat $\vvec[c] = reproj_{\vvec[w]}(\vvec[c], \vvec) + reproj_{\vvec}(\vvec[c], \vvec[w])$, hetgeen ook uit het bovenstaande volgt, rechthoekigheid even goed definieert.
Evenwel kunnen wij zonder het normbegrip geen terugkerende projecties uitbouwen, vandaar dat we de definitie in zijn relevante vorm geven.

\begin{csq}
	Als de richting van projectie rechthoekig was, is de richting van de terugkerende projectie ook rechthoekig.
	\begin{bewijs}
		Blijkt dat $\vnorm a^2\uvec[c] + \vnorm b^2\uvec[c] = \vnorm a\vnorm c\uvec[a] + \vnorm b\vnorm c\uvec[b]$ waaruit $\vnorm a\vvec[b'] = -\vnorm b\vvec[a']$ zodat alvast beide terugkerende projectierichtingen gelijk zijn.

		Voor $\vvec[r] = \vnorm a^2\vnorm c^{-2}(\vvec[a]+\vvec[b])$ geldt
		\begin{align*}
			\vnorm r^2
			 & = \vnorm a^4\vnorm c^{-4}\left(\vnorm a^2 + \vnorm b^2\right) \\
			 & = \vnorm a^4\vnorm c^{-2}
		\end{align*}
		terwijl $\vvec[p] = -\vnorm c^{-1}\vnorm a\vvec[b'] = \vvec[a] - \vvec[r]$ zodat
		\begin{align*}
			\vnorm p^2
			 & = (1 - \vnorm a^2\vnorm c^{-2})^2\vnorm a^2 + \vnorm a^4\vnorm c^{-4}\vnorm b^2 \\
			 & = \vnorm a^2 - \vnorm a^4\vnorm c^{-2}
		\end{align*}
	\end{bewijs}
\end{csq}

\begin{axm}
	De vectoren van de gekozen basis $\vecrow[t]$ zijn onderling rechthoekig.
\end{axm}

\begin{dfn}
	Samen met de bepaling dat de basisvectoren eenheidsvectoren zijn heet dit een orthonormale basis.
\end{dfn}

\begin{csq}
	Gegeven een orthonormale basis $\vecrow[t]$ bepaalt een paar vectoren $\vvec, \vvec[w]$ in beide projectiezinnen een rechte hoek om langs te projecteren.
	\begin{bewijs}
		De componenten naar de basisvectoren projecteren rechthoekig terug op een willekeurige vector.
	\end{bewijs}
\end{csq}

\begin{dfn}
	Men spreekt van een rechthoekige projectie $proj_\perp(\vvec, \vvec[w])$, of ook wel de rechthoekige component of simpelweg de component van $\vvec$ naar $\uvec[w]$.
\end{dfn}

\begin{dfn}
	Het scalair product van vectoren $\vvec$, $\vvec[w]$ ten aanzien van basis $\vecrow[t]$ is gedefinieerd als het lineair product van de rijvector met coördinaten van $\vvec$ en een kolomvector met coördinaten van $\vvec[w]$.
	\begin{equation*}
		\vvec \cdot \vvec[w] = (\cvecva[t])^T \cvecc[t]{w}
	\end{equation*}
\end{dfn}

\begin{csq}
	De norm van de rechthoekige projectie van $\vvec$ op $\vvec[w]$ is het scalair product mits schaalcorrectie voor de norm van het projectiedoel.
	\begin{equation*}
		\norm{proj_\perp(\vvec, \vvec[w])} = {\vnorm w}^{-1}\vvec \cdot \vvec[w]
	\end{equation*}
	\begin{bewijs}
		De component van $\vvec$ naar $\rvecr{t}$ is $\vcordvi{t}\rvecr{t}$. Evenwel is ${\vnorm w}^{-1}\vcordv[w]{i}{t}$ de norm van de rechthoekige projectie van $\rvecr{t}$ op $\vvec[w]$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Indien het scalair product nul is $\vvec \cdot\vvec[w] = 0$ zijn $\vvec$ en $\vvec[w]$ onderling rechthoekig $\vvec\perp\vvec[w]$.
	\begin{bewijs}
		De rechthoekige projectie van $\vvec$ op $\vvec[w]$ is de nulvector zodanig dat $\vvec$ in de richting van projectie ligt.
	\end{bewijs}
\end{csq}

\begin{csq}
	De vector rechthoekig aan het projectiedoel kan als volgt bepaald worden.
	\begin{equation*}
		\vvec[w]_\perp = \vvec - \frac{\vvec\cdot\vvec[w]}{\vnorm w^2}\vvec[w]
	\end{equation*}
	Dit volgt uit eerdere theoretische overwegingen doch kan ook rekentechnisch nagegaan worden uit $\vvec[w]_\perp\cdot\vvec[w]$.
	Merk op dat men hier desgewenst over een coördinatenrepresentatie beschikt.
\end{csq}

\begin{csq}
	De rechthoekige projectie van $\uvec$ op $\vvec[w]$ is de cosinus van de hoek $\theta$ tussen $\vvec[w]$ en $\vvec$.
	\begin{equation*}
		cos(\theta) = \frac{\vvec \cdot \vvec[w]}{\vnorm v\vnorm w}
	\end{equation*}
	Dit wordt aangetoond in het deel over affiene ruimtes.
\end{csq}

\newtheorem*{pyth}{Uitgebreide Stelling van Pythagoras}
\begin{pyth}
	Het scalair product van een vector met zichzelf is de gekwadrateerde norm $\vnorm v^2 = \vvec\cdot\vvec$.
\end{pyth}

\begin{thm}
	Het scalair product is invariant onder basistransformaties $B$ waar $B^TB=I_n$. Deze noemt men orthonormaal.
	\begin{bewijs}
		Waren $\vvec$, $\vvec[w]$ de coördinaten ten aanzien van de oorspronkelijke basis dan is het scalair product $\vvec^T B\cdot B^T\vvec[w]$ na de coördinatentransformatie.
	\end{bewijs}
\end{thm}

We bouwden ons projectiebegrip op vanuit een gekend basispaar dat de richting van projectie en die van het projectiedoel bepaalt.
We breidden het uit met een orthonormaliteitsbegrip dat ons toestaat een willekeurige vector rechthoekig op een andere te projecteren middels de basisvectoren.

Nu wensen we een vector te projecteren op een willekeurig vrij deel dat niet voortbrengend is, genaamd de projectieruimte, langs een richting vrij aan die ruimte.
Dit is simpelweg de som van de componenten naar de projectieruimte langs de projectierichting.
Merk op dat dit impliceert dat het projectieonderwerp lineair afhangt van de projectieruimte samen met de projectierichting.

Gezien deze definitie verwachten we dat vectoren in de projectieruimte, en dus ook de coördinaten daarvan naar de oorspronkelijke basis, onveranderd blijven onder een dergelijke transformatie.

Samen met een vrij deel dat de projectierichting voortbrengt doch ook vrij is van de projectieruimte, genaamd de projecterende ruimte, vormt de projectieruimte dus een basis voor de volledige ruimte.
We kunnen de basisvectoren van de projecterende ruimte zo kiezen dat het projectieonderwerp een schaal blijft van hun ongeschaalde som, zodat in de samengestelde basis de gewenste projectie af te lezen blijft van de coördinaten ten aanzien van de projectieruimte.

Indien we de coördinaten naar de samengestelde basis transformeren $B^{-1}\vvec$ kunnen we in de terugkerende transformatie de componenten naar de projecterende ruimte laten vallen $P = BI_{(n-m)+0m}B^{-1}$.
Blijkt dus zowel $PB = BI_{(n-m)+0m}$ als de idempotentie van de projectie $P^2=P$.
Uit basisvectoren voor de kolomruimte van idempotente $P$ kan anderzijds telkens een projectieruimte gevormd worden.

Hierbij is de projectierichting enkel orthogonaal als $P^T=P$.

\newpage
\end{document}