\documentclass{amsart}
\usepackage{mathtools}
\usepackage[dutch]{babel}
\usepackage[T1]{fontenc}

\theoremstyle{definition}
\newtheorem{axm}{Axioma}[section]
\newtheorem{thm}{Stelling}[section]
\newtheorem{lmm}{Lemma}[section]
\newtheorem{dfn}{Definitie}[section]
\newtheorem{csq}{Gevolg}[section]

\newenvironment{bewijs}{\begin{proof}[Bewijs]}{\end{proof}}

\newcommand{\realnums}{\mathbb{R}}
\newcommand{\realn}[1][n]{\realnums^{#1}}
\newcommand{\realmx}[2][n]{\realn[#2 \times #1]}
\newcommand{\realnxn}{\realmx{n}}
\newcommand{\realmxn}{\realmx{m}}

\newcommand{\zerovec}{\mathbf{0}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\abs}[1]{\lvert#1\rvert}

\newcommand{\setsm}[1]{\{#1\}}
\newcommand{\without}[1]{\setminus\setsm#1}
\newcommand{\with}[1]{\cup\setsm#1}

\begin{document}
\title{Lineaire algebra}
\author{Olesj V. Bilous}
%\begin{abstract}
%\end{abstract}
\maketitle

\section{Vectorruimtes}

\begin{dfn}
	Een vectorruimte $V$ bevat van elke vector $v \in V$ een unieke schaal $\lambda v$ voor elke scalair $\lambda \in \realnums$. Het bevat ook de unieke som $v + w$ van elk paar vectoren $v, w \in V$.
\end{dfn}

\begin{axm}
	De vectorschaal is associatief.
	\begin{equation*}
		\lambda (\mu v) = (\lambda\mu)v
	\end{equation*}
\end{axm}

\begin{axm}
	Er bestaat een unieke nulvector $\zerovec$ die de nulschaal $0v$ is van elke vector $v \in V$.
\end{axm}

\begin{axm}
	$1$ is de identiteitsschaal: 1v = v.
\end{axm}

\begin{axm}
	De vectorsom is associatief.
	\begin{equation*}
		(u + v) + w = u + (v + w)
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorsom is commutatief.
	\begin{equation*}
		v + w = w + v
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorschaal is distributief over de vectorsom.
	\begin{equation*}
		\lambda(v + w) = \lambda v + \lambda w
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorschaal is distributief over de scalaire som.
	\begin{equation*}
		(\lambda + \mu)v = \lambda v + \mu v
	\end{equation*}
\end{axm}

\begin{thm}
	Een lineaire ruimte is een commutatieve groep over de vectorsom.
	\begin{bewijs}
		$v + (-1)v & = (1-1)v$
	\end{bewijs}
\end{thm}

\begin{dfn}
	Een lineaire combinatie van vectoren is het lineair product van een rij vectoren $\alpha \in V^n$ en een kolom coëfficiënten $v^\alpha \in \mathbb{R}^n$.
	\begin{align*}
		\alpha v^\alpha & = \sum_i^n v^\alpha_i \alpha_i
		\\
		& = v^\alpha_1 \alpha_1 + \ldots + v^\alpha_n \alpha_n 
		\\
		& =
		\begin{pmatrix}
			\alpha_1, \ldots, \alpha_n
		\end{pmatrix}
		\begin{pmatrix}
			v^\alpha_1 \\
			\vdots       \\
			v^\alpha_n
		\end{pmatrix}
		\\
	\end{align*}
\end{dfn}
Zo is elke lineaire combinatie van vectoren $\alpha \in V^n$ ook een vector $v \in V$.

\begin{dfn}
	De schaal $\lambda v$ van een kolom coëfficiënten $v \in \realn$ is de kolom $w \in \realn$ met de schalen van de respectieve coëfficiënten.
	\begin{equation*}
		{w}_i = \lambda {v}_i
	\end{equation*}
\end{dfn}

\begin{dfn}
	De som $u + v$ van coëfficiëntkolommen $u, v \in \realn$ is de kolom $w \in \realn$ met de sommen van de respectieve coëfficiënten.
	\begin{equation*}
		{w}_i = {u}_i + {v}_i
	\end{equation*}
\end{dfn}

\begin{thm}
	$\realn$ is een vectorruimte met nulvector $0_n$.
\end{thm}

Vandaar dat men een kolom coëfficiënten doorgaans een kolomvector zal noemen. De coëfficiënten heten dan componenten.

\begin{dfn}
	De vectorrij $A \in \realn[m \times n]$ met $n$ kolomvectoren $A_{j} \in \realn[m]$ noemt men een matrix.
	Componenten van $A_j$ schrijft men als $a_{ij}$, rijvectoren als $A^i$.
\end{dfn}

\begin{dfn}
	Transpositie spiegelt een matrix om de diagonaal: $a^T_{ij} = a_{ji}$ voor $a^T \in A^T$.
\end{dfn}

\begin{csq}
	Het lineair product van een matrix $A \in \realmxn$ met een kolomvector $v \in \realn$ is een lineaire combinatie van $n$ kolomvectoren $A_i \in \realn[m]$.
\end{csq}

\begin{csq}
	Evenwel is vanwege de structuur van de kolomvectorsom elke component van $w = Av$ ook een lineaire combinatie van de componenten van $v$: $w^T = v^TA^T$.
\end{csq}

\begin{csq}
	Het herhaald lineair product van een vectorrij $\alpha \in V^m$ met de kolomvectoren $A_i$ van de matrix $A \in \realmxn$ vormt een vectorrij $\beta \in V^n$.
\end{csq}

\begin{dfn}
	Bovenstaande operatie noemen we het herhaald product van $\alpha$ en $A$.
	\begin{equation*}
		\alpha A = \beta
	\end{equation*}
\end{dfn}

\begin{dfn}
	De matrix die een vectorrij op zichzelf afbeeldt noemen we de identiteitsmatrix $I_n \in \realnxn$.
\end{dfn}

\begin{csq}
	Het herhaald product van $A \in \realmxn$ met $B \in \realn[n \times p]$ is $C \in \realn[m \times p]$.
	Dit zijn $p$ lineaire combinaties van $n$ kolomvectoren $A_i \in \realn[m]$ volgens $v$.
\end{csq}

\begin{dfn}
	Dit staat ook simpelweg als het matrixproduct bekend.
	\begin{equation*}
		AB = C
	\end{equation*}
\end{dfn}

\begin{csq}
	Het matrixproduct $AB$ bestaat anderzijds ook uit $m$ lineaire combinaties van $n$ rijvectoren $B^j \in \realn[p]$, oftewel $(AB)^T = B^TA^T$.
\end{csq}

\begin{csq}
	Het matrixproduct is associatief.
	\begin{bewijs}
		De vectorschaal is distributief over de vectorsom en bovendien associatief.
		Vandaar dat het in $ABC$ niet uitmaakt of een $C_i$ zijn coëfficiënten verdeelt over $AB$ of een $(BC)_j$ de zijne over $A$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Inverteerbare vierkante matrices zijn commutatief inverteerbaar.
	\begin{bewijs}
		Weze $AA^{-1}_r = I_n$ in $(AA^{-1}_r)A = A(A^{-1}_rA)$. Hierbij moet $A^{-1}_rA$ echter ook dezelfde identiteitsmatrix $I_n$ zijn.
	\end{bewijs}
\end{csq}

\begin{dfn}
	De spanruimte $span(\alpha)$ van een vectorrij $\alpha$ bevat alle linaire combinaties van $\alpha$.
	Men zegt dat $\alpha$ haar spanruimte voortbrengt.
\end{dfn}

\begin{csq}
	Een matrix $A \in \realmxn$ die als vectorrij voortbrengend is voor $\realn[m]$ is rechts inverteerbaar.
	Als haar rijvectoren $\realn$ voortbrengen is ze links inverteerbaar.
	\begin{bewijs}
		De identiteitsmatrices bestaan uit vectoren in de respectieve ruimtes.
	\end{bewijs}
\end{csq}

\begin{thm}
	De spanruimte van een vectorrij is een vectorruimte.
	\begin{bewijs}
		Uit $\alpha A = \beta$ en $\beta v^\beta = v$ volgt $\alpha A v^\beta = v$.
		Hieruit mag blijken dat herhaalde lineaire combinaties de spanruimte niet verlaten.
	\end{bewijs}
\end{thm}

\section{Vrije ruimtes}

Evenwel bestaan er vectorruimtes $V$ waar niet elke vector $v \in V$ een lineaire combinatie van elke vectorrij $\alpha \in V^n$ is.

\begin{dfn}
	Een vectorrij $\alpha$ is lineair onafhankelijk of vrij als geen enkele van de vectoren $\alpha_i \in \alpha$ een lineaire combinatie is van de overige $\alpha_{j\neq i} \in \alpha$.
\end{dfn}

\begin{lmm}
	Het lineair product van een vrije vectorrij met een kolomvector is enkel de nulvector $\alpha 0^\alpha = \zerovec$ als de kolomvector dat ook is in zijn respectieve ruimte: $0^\alpha = 0_n$.
	\begin{bewijs}
		$-0^\alpha_i \alpha_i = \left(\alpha \without{\alpha_i}\right)\left(0^\alpha \without{0^\alpha_i}\right)$
	\end{bewijs}
\end{lmm}

\begin{csq}
	Het herhaald product van een vrije vectorrij met een matrix is vrij als de matrix dat ook is. Als het product vrij is, is de matrix dat ook.
	\begin{bewijs}
		Stel dat $\beta = \alpha A$ lineair afhankelijk is voor $\alpha \in V^m$ vrij.
		Dan is er een $0^\beta \neq 0_n$ waar $\beta0^\beta = \zerovec$, zodanig dat $A 0^\beta = 0_n$ betekent dat ook $A$ lineair afhankelijk is.
		Anderzijds is voor $Cc = 0_k$ elke $\gamma Cc = \zerovec$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Het product van vrije matrices is vrij. Als het product vrij is, is de rechtermatrix vrij.
\end{csq}

\begin{csq}
	Een vrije vectorrij heeft voor elke vector in zijn spanruimte slechts één kolom coëfficiënten.
	\begin{bewijs}
		\begin{align*}
			\alpha v^{\alpha 1}                  & = \alpha v^{\alpha 2} \\
			\alpha (v^{\alpha 1} - v^{\alpha 2}) & = \zerovec
		\end{align*}
	\end{bewijs}
\end{csq}

\begin{dfn}
	Vanwege het unieke karakter van deze coëfficiënten noemen we ze de coördinaten van een vector met betrekking tot de vrije vectorrij.
\end{dfn}

\begin{dfn}
	Een vrije vectorrij staat gekend als de basis van haar spanruimte.
\end{dfn}

\begin{dfn}
	De coördinaten van een alternatieve basis ten aanzien van een oorspronkelijke vormen een matrix.
	Deze heet een basistransformatie.
\end{dfn}

\begin{thm}
	Als $\beta$ vrij wordt voortgebracht $span(\beta) \subseteq span(\alpha)$ uit $\alpha$ dan bevat $\beta$ ten hoogste een gelijk aantal vectoren $\abs\beta \leq \abs\alpha$.
	Als $\abs\beta = \abs\alpha$ dan is $\beta$ voortbrengend $span(\beta) = span(\alpha)$. 
	\begin{bewijs}
		Elke $\beta_j$ heeft minstens één definiete coördinaat $b_{ij} \in B$ voor een zekere $\alpha_i$.
		\begin{align*}
			\beta_j = b_{ij}\alpha_i + (\alpha \without{\alpha_i})(B_j  \without{b_{ij}}) 
		\end{align*}
		Hieruit blijkt echter dat ook deze $\alpha_i$ afhangt van  $\alpha^{(1)} = (\alpha \without{\alpha_i}) \cup \beta_j$.
		Vandaar dat ook $\alpha^{(1)}$ voortbrengend is gezien $\alpha \subseteq span(\alpha^{(1)})$.

		Daar $\beta$ vrij en $\alpha^{(m)}$ voortbrengend is heeft $\beta_k$ telkens $\alpha^{(m)}$-coördinaten buiten $\beta \without{\beta_k}$ en is deze procedure voor herhaling vatbaar tot we $\alpha$ uitputten.
		Blijft er nog enige $\beta_l \notin \alpha^{(\abs{\alpha})}$ over, dan is deze afhankelijk van $\alpha^{(\abs\alpha)} \subset \beta$ en ware $\beta$ nimmer vrij.
		Geldt evenwel $\alpha^{(\abs{\alpha})} = \beta$ dan is $\beta$ voortbrengend.
	\end{bewijs}
\end{thm}

\begin{csq}
	Elk basispaar dat dezelfde ruimte voortbrengt $span(\alpha) = span(\beta)$ heeft gelijke orde.
\end{csq}

\begin{csq}
	Een vectorrij van lagere orde dan een basis is niet voortbrengend voor de spanruimte van de basis.
\end{csq}

\begin{csq}
	Enkel vierkante matrices kunnen commutatief inverteerbaar zijn.
	\begin{bewijs}
		Weze $A \in \realmxn$ met $m < n$ doch $\realn[m]$ voortbrengend.
		Dan kan $B \in \realmx[m]{n}$ weliswaar $I_m$ vormen uit $A$, maar $A$ kan $\realn$ niet voortbrengen uit $B$ om tot $I_n$ te komen.
	\end{bewijs}
\end{csq}

\begin{csq}
	De volgende zijn gelijkwaardig voor vierkante matrices $A \in \realnxn$:

	- lineaire onafhankelijkheid

	- voortbrenging van $\realn$

	- inverteerbaarheid
\end{csq}

\begin{thm}
	Basistransformaties vormen een groep over de matrixvermenigvuldiging met de identiteitsmatrix als identiteit: $AA^{-1} = I_n = A^{-1}A$.
\end{thm}

\begin{dfn}
	Men noemt deze de algemene lineaire groep.
\end{dfn}

\begin{csq}
	Een basistransformatie is contravariant aan de ge\"{i}mpliceerde co\"{o}rdinatentransformatie. 
	\begin{bewijs}
		Uit $\alpha A = \beta$ en $\alpha v^\alpha = v$ volgt $\beta A^{-1} v^\alpha = v$.
	\end{bewijs}
\end{csq}

\section{Genormeerde ruimtes}

Nu wensen we een begrip van afstand in te voeren, gekend als de norm $\norm{v}$ van een vector $v \in V$. Hiertoe nemen we een willekeurige basis $\alpha$ van $V$ en schrijven er enkele bijzondere eigenschappen aan toe.
\begin{axm}
	Vooreerst is telkens $\norm{\alpha_i} = 1$.
\end{axm}

\begin{axm}
	Vervolgens is de schaal associatief met de norm: $\norm{\lambda v} = \lambda\norm v$.
\end{axm}

\begin{csq}
	Hieruit mag blijken dat er voor elke $v \in V$ een vector $v_e = {\norm v}^{-1}v$ waarvan de norm $\norm e = 1$.
\end{csq}

Beschouw een vrij paar vectoren $v, w$ dat een derde vector $c$ voortbrengt zodat $c = c^v v + c^w w$.

\begin{dfn}
	We noemen $c^v v$ de parallele projectie van $c$ op de richting van $v$ volgens de richting van $w$ geschreven $c^v v = proj_{w}(c, v)$.
\end{dfn}

Stel voortaan $a = proj_w(c, v)$.

\begin{csq}
	Merk op dat $\norm a$ de coördinaat is van $c$ naar $a_e$ in het basispaar $a_e, w$.
	Vandaar dat we $a$ ook de component van $c$ naar $a^e$ langs $w$ noemen.
\end{csq}


Evenwel kunnen we nu ook volgende vectoren beschouwen, $c' = \norm a c_e$ en $a' = \norm c a_e$.
Hierbij spiegelen we de driehoek gevormd door $a, c$ om de bissectrice en voltooien die met $b' = c' - a'$.

\begin{dfn}
	Evenwel wensen wij nu $a$ volgens $b'$ te projecteren op $c$, hetgeen we de terugkerende projectie van $c$ naar $v$ langs $w$ noemen $reproj_w(c, v) = proj_{b'}(a, c)$.
\end{dfn}

Blijkt evenwel dat projecties langs $b'$ coördinaten naar $a^e$ op $c^e$ schalen met een factor van $\frac{\norm a}{\norm c}$.

\begin{csq}
	\begin{equation*}
		\norm{reproj_w(c, v)} = \frac{\norm{proj_w(c, v)}^2}{\norm c}
	\end{equation*}
\end{csq}

\begin{dfn}
	Weze $\norm c$ = $\norm{reproj_w(c, v)} + \norm{reproj_v(c, w)}$ dan noemt men $v$ en $w$ onderling rechthoekig $v \perp w$.
\end{dfn}

De projecties van de aanliggende componenten raken elkaar wederom op de tegenliggende zijde.

\begin{axm}
	De vectoren van de gekozen basis $\alpha$ zijn onderling rechthoekig.
\end{axm}

\begin{dfn}
	Samen met de bepaling dat de basisvectoren eenheidsvectoren zijn heet dit een orthonormale basis.
\end{dfn}

\begin{dfn}
	Gegeven een orthonormale basis bepaalt een vrij paar vectoren $v, w$ in beide projectiezinnen een rechte hoek om langs te projecteren.
	Vandaar dat men spreekt van een rechthoekige projectie $proj_\perp(v, w)$.
\end{dfn}

\begin{dfn}
	Het scalair product van vectoren $v \cdot w$ is gedefinieerd als het lineair product $(v^\alpha)^T w^\alpha$ van een rijvector coördinaten en een coördinaatvector.
\end{dfn}

\begin{csq}
	De rechthoekige projectie van $v$ op $w$ is het scalair product mits schaalcorrectie voor de norm $proj_\perp(v, w) = {\norm w}^{-1}v \cdot w$.
	\begin{bewijs}
		De coördinaat $v^\alpha_i$ is de norm van de component van $v$ naar de basisvector $\alpha_i$. Evenwel schaalt de overeenkomstige component van de vector waar we op projecteren met een factor van $\norm w^{-1}w^\alpha_i$ in de terugkerende projectie.
	\end{bewijs}
\end{csq}

\begin{csq}
	De norm van een vector is de wortel van het scalair product met zichzelf $\norm v = \sqrt{v\cdot v}$.
\end{csq}

\newpage

\section{Special relativity}
\begin{axm}
	Linearity
	\begin{align*}
		\lambda
		\begin{pmatrix}
			a_{11} a_{12} \\
			a_{21} a_{22}
		\end{pmatrix}
		\begin{pmatrix}
			z \\
			ct
		\end{pmatrix}
		 & =
		\begin{pmatrix}
			z' \\
			ct'
		\end{pmatrix}
	\end{align*}
\end{axm}
\begin{axm}
	Scaled kinetic translation
	\begin{align*}
		z' = \mu(z - vt)
	\end{align*}
\end{axm}
\begin{axm}
	Invariance
	\begin{align*}
		\lambda^2(z^2 - c^2t^2) & = ({z'}^2 - c^2{t'}^2)
	\end{align*}
\end{axm}
\end{document}