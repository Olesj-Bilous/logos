\documentclass{amsart}
\usepackage{mathtools}
\usepackage[dutch]{babel}
\usepackage[T1]{fontenc}

%%% ENV

\theoremstyle{definition}
\newtheorem{axm}{Axioma}[section]
\newtheorem{thm}{Stelling}[section]
\newtheorem{lmm}{Lemma}[section]
\newtheorem{dfn}{Definitie}[section]
\newtheorem{csq}{Gevolg}[section]

\newenvironment{bewijs}{\begin{proof}[Bewijs]}{\end{proof}}



%%% CMD

%% sets

\newcommand{\setsm}[1]{\{{#1}\}}
\newcommand{\without}[1]{\setminus\setsm{#1}}
\newcommand{\with}[1]{\cup\setsm{#1}}

\newcommand{\realnums}{\mathbb{R}}
\newcommand{\realn}[1][n]{\realnums^{#1}}
\newcommand{\realmx}[2][n]{\realn[#2 \times #1]}
\newcommand{\realnxn}{\realmx{n}}
\newcommand{\realmxn}{\realmx{m}}

\newcommand{\vecspace}[1][v]{\mathrm{\mathbf{\MakeUppercase#1}}}
\newcommand{\vecspacen}[1][n]{\vecspace^#1}


%% numbers

\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\abs}[1]{\lvert{#1}\rvert}


%% vectors

% bold vectors
\newcommand{\vvec}[1][v]{\mathbf{#1}}
\newcommand{\uvec}[1][v]{\vvec[#1]_\mathbf{e}}

\newcommand{\vnorm}[1]{\norm{\vvec[#1]}}
\newcommand{\zerovec}{\vvec[0]}

% vector rows and their vectors
\newcommand{\vecrow}[1][a]{\mathcal{\MakeUppercase{#1}}}

\newcommand{\rvec}[2][i]{{#2}_#1}
\newcommand{\rvecr}[2][i]{\rvec[#1]{\vecrow[#2]}}
\newcommand{\rveci}[1][i]{\rvecr[#1]{a}}

% coordinate vectors
\newcommand{\cvec}[2]{{#1}^{#2}}

\newcommand{\cveca}[2][a]{\cvec{#2}{\vecrow[#1]}}

\newcommand{\cvecv}[2][v]{\cvec{\vvec[#1]}{#2}}
\newcommand{\cvecva}[1][a]{\cvecv{\vecrow[#1]}}
\newcommand{\cvecc}[2][a]{\cvecv[#2]{\vecrow[#1]}}
\newcommand{\cvecvv}[1][v]{\cvecc{#1}}

% coordinates
\newcommand{\vcord}[3]{{#1}_{#2}^{#3}}

\newcommand{\vcorda}[3][a]{\vcord{#2}{#3}{\vecrow[#1]}}
\newcommand{\vcordai}[2][i]{\vcorda{#2}{#1}}

\newcommand{\vcordv}[3][v]{\vcord{#1}{#2}{\vecrow[#3]}}
\newcommand{\vcordvi}[2][i]{\vcordv{#1}{#2}}

\newcommand{\vcordvia}[1][i]{\vcordvi[#1]{a}}


%%% DOC

\begin{document}
\title{Lineaire algebra}
\author{Olesj V. Bilous}
%\begin{abstract}
%\end{abstract}
\maketitle

\section{Vectorruimtes}


\begin{dfn}
	Een vectorruimte $\vecspace$ bevat van elke vector $\vvec \in \vecspace$ een unieke schaal $\lambda \vvec$ voor elke scalair $\lambda \in \realnums$. Het bevat ook de unieke som $\vvec + \vvec[w]$ van elk paar vectoren $\vvec, \vvec[w] \in \vecspace$.
\end{dfn}

\begin{axm}
	De vectorschaal is associatief.
	\begin{equation*}
		\lambda (\mu \vvec) = (\lambda\mu)\vvec
	\end{equation*}
\end{axm}

\begin{axm}
	$1\vvec$ is de identiteitsschaal.
	\begin{equation*}
		1\vvec=\vvec
	\end{equation*}
\end{axm}

\begin{axm}
	Er bestaat een unieke nulvector $\zerovec$ die de nulschaal is van elke vector $\vvec \in \vecspace$.
	\begin{equation*}
		0\vvec=\zerovec
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorsom is associatief.
	\begin{equation*}
		(\vvec[u] + \vvec) + \vvec[w] = \vvec[u] + (\vvec + \vvec[w])
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorsom is commutatief.
	\begin{equation*}
		\vvec + \vvec[w] = \vvec[w] + \vvec
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorschaal is distributief over de vectorsom.
	\begin{equation*}
		\lambda(\vvec + \vvec[w]) = \lambda \vvec + \lambda \vvec[w]
	\end{equation*}
\end{axm}

\begin{axm}
	De vectorschaal is distributief over de scalaire som.
	\begin{equation*}
		(\lambda + \mu)\vvec = \lambda \vvec + \mu \vvec
	\end{equation*}
\end{axm}

\begin{thm}
	Een vectorruimte is een commutatieve groep over de vectorsom.
	\begin{bewijs}
		\begin{align*}
			\vvec + 0\vvec &= (1+0)\vvec \\
			\vvec + (-1)\vvec &= (1-1)\vvec
		\end{align*}
	\end{bewijs}
\end{thm}

\begin{dfn}
	De nulruimte van een vectorruimte $\vecspace$ bepaald als $\vecspacen[0] = \setsm\zerovec$ bevat enkel de nulvector.
\end{dfn}

\begin{csq}
	De nulruimte is een triviale vectorruimte.
\end{csq}

\begin{csq}
	Een eindige deelverzameling van een vectorruimte is enkel een vectorruimte als het de triviale vectorruimte is.
\end{csq}

\begin{dfn}
	Het lineair product van een rij vectoren $\rveci \in \vecrow \in \vecspacen$ en een kolom coëfficiënten $v_i \in \cvecva \in \mathbb{R}^n$
	schaalt elke vector in de rij met de respectieve coëfficiënt in de kolom en somt over deze schalen.
	\begin{align*}
		\vecrow \cvecva &= \sum_i^n \vcordvia \rveci
		\\ & = \vcordvia[1] \rveci[1] + \ldots + \vcordvia[n] \rveci[n] 
		\\ & =
		\begin{pmatrix}
			\rveci[1], \ldots, \rveci[n]
		\end{pmatrix}
		\begin{pmatrix}
			\vcordvia[1] \\
			\vdots       \\
			\vcordvia[n]
		\end{pmatrix}
	\end{align*}
	Dit heet een lineaire combinatie van de vectoren in de rij $\vecrow$.
\end{dfn}

\begin{csq}
	Zo is elke lineaire combinatie van vectoren $\rveci \in \vecspace$ ook een vector $\vvec \in \vecspace$.
\end{csq}

\begin{dfn}
	De schaal van een kolom coëfficiënten $v_i \in \vvec \in \realn$ is de kolom $\vvec[w] \in \realn$ met de schalen van de respectieve coëfficiënten ${w}_i = \lambda {v}_i$.
	\begin{align*}
		\lambda \vvec =
		\lambda 
		\begin{pmatrix}
			v_1 \\
			\vdots \\
			v_n
		\end{pmatrix}
		= 
		\begin{pmatrix}
			\lambda v_1 \\
			\vdots \\
			\lambda v_n
		\end{pmatrix}
	\end{align*}
\end{dfn}

\begin{dfn}
	De som van coëfficiëntkolommen $\vvec[u], \vvec \in \realn$ is de kolom $\vvec[w] \in \realn$ met de sommen van de respectieve coëfficiënten ${w}_i = {u}_i + {v}_i$.
	\begin{align*}
		\vvec[u] + \vvec =
		\begin{pmatrix}
			u_1 \\
			\vdots \\
			u_n
		\end{pmatrix}
		+
		\begin{pmatrix}
			v_1 \\
			\vdots \\
			v_n
		\end{pmatrix}
		=
		\begin{pmatrix}
			u_1 + v_1 \\
			\vdots \\
			u_1 + v_n
		\end{pmatrix}
	\end{align*}
\end{dfn}

\begin{thm}
	$\realn$ is een vectorruimte met nulvector $0_n$ waar elke coëfficiënt nul is.
\end{thm}

Vandaar dat men een kolom coëfficiënten doorgaans een kolomvector zal noemen. De coëfficiënten heten dan componenten.

\begin{dfn}
	De vectorrij $A \in \realn[m \times n]$ met $n$ kolomvectoren $A_{j} \in \realn[m]$ noemt men een matrix.

	Evenwel vormt de matrix $A$ ook een vectorkolom met $m$ rijvectoren $A^i \in \realn$. 

	De gemene component tussen $A_j$ en $A^i$ noemt men een element van de matrix, geschreven $a_{ij}$.
\end{dfn}

\begin{dfn}
	Transpositie $A^T$ spiegelt een matrix $A$ om de diagonaal $A^T_j = A^i$ zodat $a^T_{ij} = a_{ji}$.
\end{dfn}

\begin{csq}
	Het lineair product $A\vvec$ van een matrix $A \in \realmxn$ met een kolomvector $\vvec \in \realn$ is een lineaire combinatie van $n$ kolomvectoren $A_i \in \realn[m]$.
\end{csq}

\begin{csq}
	Evenwel is vanwege de structuur van het lineair product en de kolomvectorsom elke component van $\vvec[w] = A\vvec$ ook een lineaire combinatie van de componenten van $\vvec$, zodat $\vvec[w]^T = \vvec^TA^T$.
\end{csq}

\begin{csq}
	Het herhaald lineair product van een vectorrij $\vecrow \in \vecspacen[m]$ met de kolomvectoren $B_i$ van de matrix $B \in \realmxn$ vormt een vectorrij $\vecrow[b] \in \vecspacen$.
\end{csq}

\begin{dfn}
	Bovenstaande operatie noemen we het herhaald product van $\vecrow$ en $B$.
	\begin{equation*}
		\vecrow B = \vecrow[b]
	\end{equation*}
\end{dfn}

\begin{dfn}
	De matrix die een vectorrij op zichzelf afbeeldt noemen we de identiteitsmatrix $I_n \in \realnxn$.
\end{dfn}

\begin{csq}
	Het herhaald product van $A \in \realmx[m]{k}$ met $B \in \realmxn$ is $C \in \realmx[n]{k}$.
	Dit zijn $n$ lineaire combinaties van $m$ kolomvectoren $A_i \in \realn[k]$ volgens $v$.
\end{csq}

\begin{dfn}
	Dit staat ook simpelweg als het matrixproduct bekend.
	\begin{equation*}
		AB = C
	\end{equation*}
\end{dfn}

\begin{csq}
	Het matrixproduct $AB$ bestaat anderzijds ook uit $k$ lineaire combinaties van $m$ rijvectoren $B^j \in \realn[n]$, oftewel $(AB)^T = B^TA^T$.
\end{csq}

\begin{csq}
	Het matrixproduct is associatief.
	\begin{bewijs}
		De vectorschaal is distributief over de vectorsom en bovendien associatief.
		Vandaar dat het in $ABC$ niet uitmaakt of een $C_i$ zijn coëfficiënten verdeelt over $AB$ of een $(BC)_j$ de zijne over $A$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Inverteerbare vierkante matrices zijn commutatief inverteerbaar.
	\begin{bewijs}
		Weze $AA^{-1}_r = I_n$ in $(AA^{-1}_r)A = A(A^{-1}_rA)$. Hierbij moet $A^{-1}_rA$ echter ook dezelfde identiteitsmatrix $I_n$ zijn.
	\end{bewijs}
\end{csq}

\begin{dfn}
	De spanruimte $span(\vecrow)$ van een vectorrij $\vecrow$ bevat alle linaire combinaties van $\vecrow$.
	Men zegt dat $\vecrow$ haar spanruimte voortbrengt.
\end{dfn}

\begin{csq}
	Een matrix $A \in \realmxn$ die als vectorrij voortbrengend is voor $\realn[m]$ is rechts inverteerbaar.
	Als haar rijvectoren $\realn$ voortbrengen is ze links inverteerbaar.
	\begin{bewijs}
		De identiteitsmatrices bestaan uit vectoren in de respectieve ruimtes.
	\end{bewijs}
\end{csq}

\begin{thm}
	De spanruimte van een vectorrij is een vectorruimte.
	\begin{bewijs}
		Uit $\vecrow B = \vecrow[b]$ en $\vecrow[b] v^{\vecrow[b]}= v$ volgt $\vecrow B v^{\vecrow[b]}= v$.
		Hieruit mag blijken dat herhaalde lineaire combinaties de spanruimte niet verlaten.
	\end{bewijs}
\end{thm}

\section{Vrije ruimtes}

Evenwel bestaan er vectorruimtes $\vecspace$ waar niet elke vector $\vvec \in \vecspace$ een lineaire combinatie van elke vectorrij $\vecrow \in \vecspacen$ is.

\begin{dfn}
	Een vectorrij $\vecrow$ is lineair onafhankelijk of vrij als geen enkele van de vectoren $\vecrow_i \in \vecrow$ een lineaire combinatie is van de overige $\vecrow_{j\neq i} \in \vecrow$.
\end{dfn}

\begin{lmm}
	Het lineair product van een vrije vectorrij met een kolomvector is enkel de nulvector $\vecrow \cvecvv[0] = \zerovec$ als de kolomvector dat ook is in zijn respectieve ruimte: $\cvecvv[0] = 0_n$.
	\begin{bewijs}
		$-\vcordai 0 \vecrow_i = \left(\vecrow \without{\rveci}\right)\left(\cvecvv[0] \without{\vcordai 0}\right)$
	\end{bewijs}
\end{lmm}

\begin{csq}
	Het herhaald product van een vrije vectorrij met een matrix is vrij als de matrix dat ook is. Als het product vrij is, is de matrix dat ook.
	\begin{bewijs}
		Stel dat $\vecrow[b] = \vecrow B$ lineair afhankelijk is voor $\vecrow \in \vecspacen[m]$ vrij.
		Dan is er een $\cvecc[b]{0} \neq 0_n$ waar $\vecrow[b] \cvecc[b]{0} = \zerovec$, waaruit $A \cvecc[b]{0} = 0_n$ zodat ook $A$ lineair afhankelijk is.
		Anderzijds is voor $D\vvec = 0_k$ elke $\vecrow[c] D\vvec = \zerovec$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Het product van vrije matrices is vrij. Als het product vrij is, is de rechtermatrix vrij.
\end{csq}

\begin{csq}
	Een vrije vectorrij heeft voor elke vector in zijn spanruimte slechts één kolom coëfficiënten.
	\begin{bewijs}
		\begin{align*}
			\vecrow \vvec                 & = \vecrow \vvec[w] \\
			\vecrow (\vvec - \vvec[w]) & = \zerovec
		\end{align*}
	\end{bewijs}
\end{csq}

\begin{dfn}
	Vanwege het unieke karakter van deze coëfficiënten noemen we ze de coördinaten van een vector met betrekking tot de vrije vectorrij.
\end{dfn}

\begin{dfn}
	Een vrije vectorrij staat gekend als de basis van haar spanruimte.
\end{dfn}

\begin{dfn}
	De coördinaten van een alternatieve basis ten aanzien van een oorspronkelijke vormen een matrix.
	Deze heet een basistransformatie.
\end{dfn}

\begin{thm}
	Als $\vecrow[b]$ vrij wordt voortgebracht $span(\vecrow[b]) \subseteq span(\vecrow)$ uit $\vecrow$ dan bevat $\vecrow[b]$ ten hoogste een gelijk aantal vectoren $\abs{\vecrow[b]} \leq \abs\vecrow$.
	Als $\abs{\vecrow[b]} = \abs\vecrow$ dan is $\vecrow[b]$ voortbrengend $span(\vecrow[b]) = span(\vecrow)$ en $\vecrow$ vrij. 
	\begin{bewijs}
		Elke $\vecrow[b]_j$ heeft minstens één definiete coëfficiënt $b_{ij} \in B$ voor een zekere $\vecrow_i$.
		\begin{align*}
			\vecrow[b]_j = b_{ij}\vecrow_i + (\vecrow \without{\vecrow_i})(B_j  \without{b_{ij}}) 
		\end{align*}
		Hieruit blijkt echter dat ook deze $\vecrow_i$ afhangt van  $\vecrow^{(1)} = \left(\vecrow \without{\vecrow_i}\right) \cup \rvecr[j]{b}$.
		Vandaar dat $\vecrow^{(1)}$ voortbrengend blijft gezien $span(\vecrow) \subseteq span(\vecrow^{(1)})$.

		Daar $\vecrow[b]$ vrij is en voortgebracht door $\vecrow^{(m)}$ heeft elke $\rvecr[k]{b}$ telkens definiete $\vecrow^{(m)}$-coëfficiënten buiten $\vecrow[b] \without{\rvecr[k]{b}}$
		en is deze procedure voor herhaling vatbaar tot we $\vecrow$ uitputten.

		Blijft er nog enige $\vecrow[b]_l \notin \vecrow^{(\abs{\vecrow})}$ over,
		dan is deze lineair afhankelijk van $\vecrow^{(\abs\vecrow)} \subset \vecrow[b]$ en ware $\vecrow[b]$ nimmer vrij.

		Geldt evenwel $\vecrow^{(\abs{\vecrow})} = \vecrow[b]$ dan is $\vecrow[b]$ voortbrengend.
		Ware $\vecrow$ nu niet vrij, dan kon ze ingeperkt worden zonder aan de voortbrengendheid te schaden.
		We toonden evenwel juist aan dat een vrij voorgebracht deel niet groter kan zijn dan het voortbrengende.
	\end{bewijs}
\end{thm}

\begin{csq}
	Elk basispaar dat dezelfde vectorruimte voortbrengt $span(\vecrow) = span(\vecrow[b])$ heeft gelijke orde $\abs\vecrow = \abs{\vecrow[b]}$.
	We noemen deze orde de dimensie van de vectorruimte.
\end{csq}

\begin{csq}
	Een vectorrij $\vecrow$ van lagere orde $\abs\vecrow < \abs{\vecrow[b]}$ dan een basis $\vecrow[b]$ is niet voortbrengend voor de spanruimte van de basis $span(\vecrow) \subset span(\vecrow[b])$.
\end{csq}

\begin{csq}
	Enkel vierkante matrices kunnen commutatief inverteerbaar zijn.
	\begin{bewijs}
		Weze $A \in \realmxn$ met $m < n$ doch $\realn[m]$ voortbrengend.
		Dan kan $B \in \realmx[m]{n}$ weliswaar $I_m$ vormen uit $A$, maar $A$ kan $\realn$ niet voortbrengen uit $B$ om tot $I_n$ te komen.
	\end{bewijs}
\end{csq}

\begin{csq}
	De volgende zijn gelijkwaardig voor vierkante matrices $A \in \realnxn$:

	- lineaire onafhankelijkheid van zowel de vectorrij als de vectorkolom

	- voortbrenging van $\realn$ uit zowel de vectorrij als de vectorkolom

	- inverteerbaarheid
\end{csq}

\begin{thm}
	Basistransformaties vormen een groep over de matrixvermenigvuldiging met de identiteitsmatrix als identiteit: $AA^{-1} = I_n = A^{-1}A$.
\end{thm}

\begin{dfn}
	Men noemt deze de algemene lineaire groep.
\end{dfn}

\begin{csq}
	Een basistransformatie is contravariant aan de ge\"{i}mpliceerde co\"{o}rdinatentransformatie.
	\begin{bewijs}
		Uit $\vecrow A = \vecrow[b]$ en $\vecrow v^{\vecrow}= v$ volgt $\vecrow[b] A^{-1} v^{\vecrow}= v$. 
	\end{bewijs}
\end{csq}

Evenwel betekent dit slechts dat de overeenkomstige matrices elkaars inverse zijn.
De wijze waarop een matrix inwerkt op een vectorrij verschilt immers van de werking op een kolom coördinaten, waarbij commutativiteit niet gegarandeerd is.

\section{Genormeerde ruimtes}

Nu wensen we een begrip van afstand in te voeren, gekend als de norm $\vnorm v$ van een vector $\vvec \in \vecspace$.
Hiertoe nemen we een willekeurige basis $\vecrow[t]$ van $\vecspace$ en schrijven er enkele bijzondere eigenschappen aan toe.
\begin{axm}
	Vooreerst is telkens $\norm{\rvecr{t}} = 1$.
\end{axm}

\begin{axm}
	Vervolgens is de schaal associatief met de norm: $\norm{\lambda \vvec} = \lambda\vnorm v$.
\end{axm}

\begin{csq}
	Hieruit mag blijken dat er voor elke $\vvec \in \vecspace$ een vector $\uvec = {\vnorm v}^{-1}\vvec$ bestaat waarvan de norm $\vnorm \uvec = 1$.
\end{csq}

Beschouw een vrij paar vectoren $\vvec, \vvec[w]$ dat een derde vector $\vvec[c]$ voortbrengt zodat $\vvec[c] = \lambda \vvec + \mu \vvec[w]$.

\begin{dfn}
	We noemen $\lambda\vvec$ de parallele projectie van $\vvec[c]$ op de richting van $\vvec$ volgens de richting van $\vvec[w]$ geschreven $proj_{\vvec[w]}(\vvec[c], \vvec)$.
\end{dfn}

Stel voortaan $\vvec[a] = proj_{\vvec[w]}(\vvec[c], \vvec)$.

\begin{csq}
	Merk op dat $\vnorm{a}$ de coördinaat is van $\vvec[c]$ naar $\uvec[a]$ in het basispaar $\uvec[a], \vvec[w]$.
	Vandaar dat we $\vvec[a]$ ook de component van $\vvec[c]$ naar $\uvec[a]$ langs $\vvec[w]$ noemen.
\end{csq}

\begin{csq}
	Als $\vvec[a] = \zerovec$ dan ligt $\vvec[c]$ in de richting van projectie.
	\begin{bewijs}
		Daar $\lambda$ nul blijkt is $\vvec[c] = \mu\vvec[w]$ een schaal van $\vvec[w]$.
	\end{bewijs}
\end{csq}

\begin{csq}
	De parallelle projectie van een vector is gelijk aan de som van de parallele projecties van de componenten van die vector.
	\begin{bewijs}
		Weze $\vvec[c] = \sum_{i} \nu_i{\vvec[u]}_i$ voor willekeurige vectoren $\vvec[u]_i$.
		Elke ${\vvec[u]}_i$ kan geschreven worden als een lineaire combinatie van een basis die $\vvec$, $\vvec[w]$ bevat, waarbij ze dus een coördinaat $\omega_i$ hebben naar $\vvec$.
		Dan is de coördinaat van $\vvec[c]$ naar $\vvec$ in die basis gelijk aan $\sum_i \nu_i\omega_i$.
	\end{bewijs}
\end{csq}

Evenwel kunnen we nu ook volgende vectoren beschouwen, $\vvec[c'] = \vnorm a \uvec[c]$ en $\vvec[a'] = \vnorm c \uvec[a]$.
Hierbij spiegelen we de driehoek gevormd door $\vvec[a], \vvec[c]$ om de bissectrice en voltooien die met $\vvec[b'] = \vvec[c'] - \vvec[a']$.

\begin{dfn}
	Evenwel wensen wij nu $\vvec[a]$ volgens $\vvec[b']$ te projecteren op $\vvec[c]$, hetgeen we de terugkerende projectie van $\vvec[c]$ naar $\vvec$ langs $\vvec[w]$ noemen $reproj_{\vvec[w]}(\vvec[c], \vvec) = proj_{\vvec[b']}(\vvec[a], \vvec[c])$.
\end{dfn}

Blijkt evenwel uit de definitie van $\vvec[b']$ dat projecties langs $\vvec[b']$ op $\vvec[c]$ van een component naar $\uvec[a]$ schalen met een factor van $\frac{\vnorm a}{\vnorm c}$.

\begin{csq}
	\begin{equation*}
		\norm{reproj_{\vvec[w]}(\vvec[c], \vvec)} = \frac{\norm{proj_{\vvec[w]}(\vvec[c], \vvec)}^2}{\vnorm c}
	\end{equation*}
\end{csq}

\begin{dfn}
	Weze $\vnorm c$ = $\norm{reproj_{\vvec[w]}(\vvec[c], \vvec)} + \norm{reproj_{\vvec}(\vvec[c], \vvec[w])}$ dan noemt men $\vvec$ en $\vvec[w]$ onderling rechthoekig $\vvec \perp \vvec[w]$.
\end{dfn}

Merk op dat $\vvec[c] = reproj_{\vvec[w]}(\vvec[c], \vvec) + reproj_{\vvec}(\vvec[c], \vvec[w])$ rechthoekigheid even goed definieert.
Evenwel kunnen wij zonder het normbegrip geen terugkerende projecties uitbouwen, vandaar dat we de definitie in zijn relevante vorm geven.

\begin{axm}
	De vectoren van de gekozen basis $\vecrow[t]$ zijn onderling rechthoekig.
\end{axm}

\begin{dfn}
	Samen met de bepaling dat de basisvectoren eenheidsvectoren zijn heet dit een orthonormale basis.
\end{dfn}

\begin{dfn}
	Gegeven een orthonormale basis bepaalt een vrij paar vectoren $\vvec, \vvec[w]$ in beide projectiezinnen een rechte hoek om langs te projecteren.
	Vandaar dat men spreekt van een rechthoekige projectie $proj_\perp(\vvec, \vvec[w])$.
\end{dfn}

\begin{dfn}
	Het scalair product van vectoren $\vvec$, $\vvec[w]$ is gedefinieerd als het lineair product $\vvec \cdot \vvec[w] = (\cvecva[t])^T \cvecc[t]{w}$ van de rijvector met coördinaten van $\vvec$ en een kolomvector met coördinaten van $\vvec[w]$.
\end{dfn}

\begin{csq}
	De rechthoekige projectie van $\vvec$ op $\vvec[w]$ is het scalair product mits schaalcorrectie voor de norm van het projectiedoel $proj_\perp(\vvec, \vvec[w]) = {\vnorm w}^{-1}\vvec \cdot \vvec[w]$.
	\begin{bewijs}
		De component van $\vvec$ naar $\rvecr{t}$ is $\vcordvi{t}\rvecr{t}$. Evenwel is $\vcordv[w]{i}{t}{\vnorm w}^{-1}$ de norm van de rechthoekige projectie van $\rvecr{t}$ op $\vvec[w]$.
	\end{bewijs}
\end{csq}

\begin{csq}
	Indien het scalair product nul is $\vvec \cdot\vvec[w] = 0$ zijn $\vvec$ en $\vvec[w]$ onderling rechthoekig $\vvec\perp\vvec[w]$.
	\begin{bewijs}
		De rechthoekige projectie van $\vvec$ naar $\vvec[w]$ is de nulvector zodanig dat $\vvec$ in de richting van projectie ligt.
	\end{bewijs}
\end{csq}

\begin{dfn}
	De rechthoekige projectie van $\uvec$ op $\vvec[w]$ heet de cosinus van de hoek $\theta$ tussen $\vvec[w]$ en $\vvec$.
	\begin{equation*}
		cos(\theta) = \frac{\vvec \cdot \vvec[w]}{\vnorm v\vnorm w}
	\end{equation*}
\end{dfn}

\begin{csq}
	De norm van een vector is de wortel van het scalair product met zichzelf $\vnorm v = \sqrt{\vvec\cdot\vvec}$.
\end{csq}

\newpage

\section{Special relativity}
\begin{axm}
	Linearity
	\begin{align*}
		\lambda
		\begin{pmatrix}
			a_{11} a_{12} \\
			a_{21} a_{22}
		\end{pmatrix}
		\begin{pmatrix}
			z \\
			ct
		\end{pmatrix}
		 & =
		\begin{pmatrix}
			z' \\
			ct'
		\end{pmatrix}
	\end{align*}
\end{axm}
\begin{axm}
	Scaled kinetic translation
	\begin{align*}
		z' = \mu(z - vt)
	\end{align*}
\end{axm}
\begin{axm}
	Invariance
	\begin{align*}
		\lambda^2(z^2 - c^2t^2) & = ({z'}^2 - c^2{t'}^2)
	\end{align*}
\end{axm}
\end{document}